{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4WjyyQd0liFV"
      },
      "outputs": [],
      "source": [
        "#Multi-Modal Explainanble customer churn prediction system"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtbvS8VbKMRg",
        "outputId": "d9a71694-42f8-4f72-ac3a-931e0f296329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.48.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from shap) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from shap) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from shap) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from shap) (25.0)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from shap) (4.14.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->shap) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers transformers torch torchvision\n",
        "!pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TS5pPgH8mJZ4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pShxsfEQneju"
      },
      "outputs": [],
      "source": [
        "#Data Generation and preprocessing\n",
        "class DataGenerator:\n",
        "  #GENERATE SYNTHETIC MULTI-MODAL CUSTOMER DATA FOR DEMO\n",
        "  def __init__(self, n_customers=10000, sequence_length=30, random_state=42):\n",
        "    np.random.seed(random_state)\n",
        "    torch.manual_seed(random_state)\n",
        "    self.n_customers = n_customers\n",
        "    self.sequence_length = sequence_length\n",
        "\n",
        "  def generate_behavioural_sequences(self):\n",
        "    #Generate behavioural sequences (e.g, daily app usage, clicks, time spent)\n",
        "    #generate patterns for churned vs non churned users\n",
        "    behavioural_data = []\n",
        "    labels = []\n",
        "\n",
        "    for i in range(self.n_customers):\n",
        "      # determine if customer will churn (30% churn rate)\n",
        "      will_churn = np.random.random() < 0.3\n",
        "\n",
        "      if will_churn:\n",
        "        #Declining engagement pattern\n",
        "        base_activity = np.random.exponential(2, self.sequence_length)\n",
        "        decay_factor = np.linspace(1, 0.1, self.sequence_length)\n",
        "        activity = base_activity * decay_factor\n",
        "\n",
        "      else:\n",
        "        #stable or increasing engagement\n",
        "        base_activity = np.random.exponential(3, self.sequence_length)\n",
        "        growth_factor = np.linspace(0.1, 1.2, self.sequence_length)\n",
        "        activity = base_activity * growth_factor\n",
        "\n",
        "      #Add noise and multiple behavioural features\n",
        "      features = np.column_stack([\n",
        "          activity, #daily usage hours\n",
        "          np.random.poisson(activity * 5), #daily clicks\n",
        "          np.random.exponential(activity), # session_duration\n",
        "          np.random.binomial(1, activity / np.max(activity)),\n",
        "      ])\n",
        "\n",
        "      behavioural_data.append(features)\n",
        "      labels.append(int(will_churn))\n",
        "\n",
        "    return np.array(behavioural_data), np.array(labels)\n",
        "\n",
        "  def generate_textual_data(self, labels):\n",
        "    #generate customer support logs and feedback text\n",
        "     positive_phrases = [\n",
        "         \"Great service, love the features\",\n",
        "            \"Excellent experience, highly recommend\",\n",
        "            \"Amazing platform, very satisfied\",\n",
        "            \"Outstanding support team\",\n",
        "            \"Perfect for my needs\"\n",
        "     ]\n",
        "\n",
        "     negative_phrases = [\n",
        "          \"Having issues with the platform\",\n",
        "            \"Disappointed with recent changes\",\n",
        "            \"Considering canceling subscription\",\n",
        "            \"Too expensive for what it offers\",\n",
        "            \"Technical problems persist\"\n",
        "      ]\n",
        "\n",
        "     textual_data = []\n",
        "     for label in labels:\n",
        "        if label == 1: # churned cus\n",
        "            text = np.random.choice(negative_phrases)\n",
        "        else: # Non-Churned\n",
        "            text = np.random.choice(positive_phrases)\n",
        "\n",
        "        #add some randomness\n",
        "        if np.random.random() < 0.2:\n",
        "           #20% chance of opposite sentiment\n",
        "           if label == 1:\n",
        "            text = np.random.choice(positive_phrases)\n",
        "           else:\n",
        "            text = np.random.choice(negative_phrases)\n",
        "\n",
        "        textual_data.append(text)\n",
        "\n",
        "     return np.array(textual_data)\n",
        "\n",
        "  def generate_structured_data(self, labels):\n",
        "    #generate structural customer features\n",
        "    structured_data = []\n",
        "\n",
        "    for label in labels:\n",
        "      #create correlated features with churn\n",
        "      if label == 1: # churned cus\n",
        "          tenure = np.random.exponential(5) #shorter tenure\n",
        "          monthly_changes = np.random.normal(80, 20) #higher charges\n",
        "          support_tickets = np.random.poisson(3) #more tickets\n",
        "          plan_changes = np.random.poisson(2) #more plan changes\n",
        "      else: # Non-Churned\n",
        "          tenure = np.random.exponential(15) #longer tenure\n",
        "          monthly_changes = np.random.normal(50, 15) #lower charges\n",
        "          support_tickets = np.random.poisson(1) #less tickets\n",
        "          plan_changes = np.random.poisson(0.5) #less plan changes\n",
        "\n",
        "      features = [\n",
        "          tenure,\n",
        "          monthly_changes,\n",
        "          support_tickets,\n",
        "          plan_changes,\n",
        "          np.random.choice([0,1]), #auto pay\n",
        "          np.random.choice([0, 1, 2]) #contract type\n",
        "      ]\n",
        "\n",
        "      structured_data.append(features)\n",
        "\n",
        "    return np.array(structured_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yzB7wqryx81x"
      },
      "outputs": [],
      "source": [
        "#Self supervised learning component\n",
        "class Time2Vec(nn.Module):\n",
        "  #Time2Vec encoding for temporal sequences\n",
        "\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(Time2Vec, self).__init__()\n",
        "    self.linear = nn.Linear(input_dim, output_dim - 1)\n",
        "    self.periodic = nn.Linear(input_dim, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x shape: (batch_size, seq_len, input_dim)\n",
        "    linear_out = self.linear(x)\n",
        "    periodic_out = torch.sin(self.periodic(x))\n",
        "    return torch.cat([linear_out, periodic_out], dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ucCMun6oznPM"
      },
      "outputs": [],
      "source": [
        "from operator import neg\n",
        "class SimCLRBehavioural(nn.Module):\n",
        "  #SimClR- inspired self-supervised learning for behavioural sequences\n",
        "\n",
        "  def __init__(self, input_dim, hidden_dim=128, projection_dim=64):\n",
        "    super(SimCLRBehavioural, self).__init__()\n",
        "\n",
        "    #encoder\n",
        "    self.encoder = nn.Sequential(\n",
        "        nn.Linear(input_dim, hidden_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_dim, hidden_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_dim, projection_dim)\n",
        "\n",
        "    )\n",
        "\n",
        "    #projection head\n",
        "    self.projection_head = nn.Sequential(\n",
        "        nn.Linear(projection_dim, hidden_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_dim, projection_dim)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    #x shape: (batch_size, seq_len, input_dim)\n",
        "    #pool over sequence dimension\n",
        "    x_pooled = torch.mean(x, dim=1)\n",
        "\n",
        "    #encode\n",
        "    encoded = self.encoder(x_pooled)\n",
        "\n",
        "    #project\n",
        "    projected = self.projection_head(encoded)\n",
        "\n",
        "    return encoded, projected\n",
        "\n",
        "  def contrastive_loss(self, z1, z2, temperature=0.5):\n",
        "    #calculate contrastive loss btw pairs\n",
        "    batch_size = z1.size[0]\n",
        "\n",
        "    #normalize embeddings\n",
        "    z1_norm = F.normalize(z1, dim=1)\n",
        "    z2_norm = F.normalize(z2, dim=1)\n",
        "\n",
        "    #compute similarity matrix\n",
        "    sim_matrix = torch.matmul(z1_norm, z2_norm.T / temperature)\n",
        "\n",
        "    #create positive mask\n",
        "    pos_mask = torch.eye(batch_size, device=z1.device).bool()\n",
        "\n",
        "    #compute loss\n",
        "    exp_sim = torch.exp(sim_matrix)\n",
        "    pos_sim = exp_sim[pos_mask]\n",
        "    neg_sim = exp_sim[pos_mask].view(batch_size, -1).sum(dim=1)\n",
        "\n",
        "    loss = -torch.log(pos_sim / (pos_sim + neg_sim))\n",
        "\n",
        "    return loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Z0W7thBq5D2Q"
      },
      "outputs": [],
      "source": [
        "#Temporal transformer architecture\n",
        "class TemporalTransformer(nn.Module):\n",
        "  #transformer with Time2Vec for sequential behavioural modeling\n",
        "  def __init__(self, input_dim, d_model=128, nhead=8, num_layers=3, dropout=0.1):\n",
        "    super(TemporalTransformer, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.input_projections = nn.Linear(input_dim, d_model)\n",
        "    self.time2vec = Time2Vec(input_dim, d_model)\n",
        "\n",
        "    #Transformer layers\n",
        "    encoder_layer = nn.TransformerEncoderLayer(\n",
        "        d_model=d_model,\n",
        "        nhead=nhead,\n",
        "        dim_feedforward= d_model * 4,\n",
        "        dropout=dropout,\n",
        "        batch_first=True\n",
        "    )\n",
        "    self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "    #output projection\n",
        "    self.output_projection = nn.Linear(d_model, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #x shape: (batch_size, seq_len, input_dim)\n",
        "    batch_size, seq_len, input_dim = x.shape\n",
        "\n",
        "    #apply TimeVec encoding\n",
        "    time_encoded = self.time2vec(x)\n",
        "\n",
        "    #apply transformer\n",
        "    transformer_out = self.transformer(time_encoded)\n",
        "\n",
        "    #global average pooling\n",
        "    pooled = torch.mean(transformer_out, dim=1)\n",
        "\n",
        "    #output projection\n",
        "    output = self.output_projection(pooled)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "leXStZwikFeo"
      },
      "outputs": [],
      "source": [
        "#Textual feature extraction\n",
        "class TextualFeatureExtractor:\n",
        "  #extract sentiment and intent features from customer text using LLM embeddings\n",
        "\n",
        "  def __init__(self, model_name = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
        "    self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    self.model = AutoModel.from_pretrained(model_name)\n",
        "    self.model.eval()\n",
        "\n",
        "  def extract_features(self, texts, batch_size=32):\n",
        "    #extract embeddings from text data\n",
        "    all_embeddings = []\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "      batch_text = texts[i:i+batch_size]\n",
        "\n",
        "      #tokenize\n",
        "      inputs = self.tokenizer(\n",
        "          batch_text,\n",
        "          padding=True,\n",
        "          truncation=True,\n",
        "          max_length=512,\n",
        "          return_tensors=\"pt\"\n",
        "      )\n",
        "\n",
        "      #extract embeddings\n",
        "      with torch.no_grad():\n",
        "        outputs = self.model(**inputs)\n",
        "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "        all_embeddings.append(embeddings)\n",
        "\n",
        "    return torch.cat(all_embeddings, dim=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KdaHUoQGHjkf"
      },
      "outputs": [],
      "source": [
        "#Multi-Modal Fusion Model\n",
        "class MultiModalChurnPredictor(nn.Module):\n",
        "  #Late-Fusion multi-modal model for churn prediction\n",
        "  def __init__(self, behavioural_dim=4, textual_dim=384, structured_dim=6,hidden_dim=128, dropout=0.2):\n",
        "    super(MultiModalChurnPredictor, self).__init__()\n",
        "\n",
        "    #Behavioural Pathway\n",
        "    self.behavioural_transformer = TemporalTransformer(input_dim=behavioural_dim, d_model=hidden_dim)\n",
        "\n",
        "    #Textual Pathway\n",
        "    self.textual_projection = nn.Sequential(\n",
        "        nn.Linear(textual_dim, hidden_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(hidden_dim, hidden_dim)\n",
        "    )\n",
        "\n",
        "    #Structured Pathway\n",
        "    self.structured_projection = nn.Sequential(\n",
        "        nn.Linear(structured_dim, hidden_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(hidden_dim, hidden_dim)\n",
        "    )\n",
        "\n",
        "    #fusion and classification\n",
        "    self.fusion = nn.Sequential(\n",
        "        nn.Linear(hidden_dim * 3, hidden_dim*2),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(hidden_dim * 2, hidden_dim),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(hidden_dim, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, behavioural_seq, textual_emb, structured_features):\n",
        "    #Process each modality\n",
        "    behavioural_repr = self.behavioural_transformer(behavioural_seq)\n",
        "    textual_repr = self.textual_projection(textual_emb)\n",
        "    structured_repr = self.structured_projection(structured_features)\n",
        "\n",
        "    #late fusion\n",
        "    fused_repr = torch.cat([behavioural_repr, textual_repr, structured_repr], dim=1)\n",
        "\n",
        "    #classification\n",
        "    output = self.fusion(fused_repr)\n",
        "\n",
        "    return torch.sigmoid(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Y5VwQxwqMSq3"
      },
      "outputs": [],
      "source": [
        "#Explainability Components\n",
        "class ChurnExplainer:\n",
        "  #SHAP-based explainability for churn predictions\n",
        "\n",
        "  def __init__(self, model, feature_names):\n",
        "    self.model = model\n",
        "    self.feature_names = feature_names\n",
        "\n",
        "  def explain_predictions(self, data_sample, background_data=None):\n",
        "    #Generate SHAP explainations for predictions\n",
        "\n",
        "    #create wrapper function for SHAP\n",
        "   def model_wrapper(x):\n",
        "    #convert to tensors and make predictions\n",
        "    behavioural_seq = torch.tensor(x[:, :120].reshape(-1, 30, 4), dtype=torch.float32)\n",
        "    textual_emb = torch.tensor(x[:, 120:504], dtype=torch.float32)\n",
        "    structured_features = torch.tensor(x[:, 504:], dtype=torch.float32)\n",
        "\n",
        "\n",
        "    self.model.eval()\n",
        "    with torch.no_grad():\n",
        "      predictions = self.model(behavioural_seq.to(self.model.fusion[0].weight.device), textual_emb.to(self.model.fusion[0].weight.device), structured_features.to(self.model.fusion[0].weight.device))\n",
        "\n",
        "    return predictions.cpu().numpy()\n",
        "\n",
        "    #USE SHAP KERNELEXPLAINER\n",
        "    if background_data is None:\n",
        "      background_data = data_sample[:100] #use subset as background\n",
        "\n",
        "    explainer = shap.KernelExplainer(model_wrapper, background_data)\n",
        "    shap_values = explainer.shap_values(data_sample)\n",
        "\n",
        "    return shap_values\n",
        "\n",
        "   def generate_counterfactuals(self, instance, target_prob=0.3, max_iterations=100):\n",
        "    #Generate counterfactual explanations\n",
        "    #simple gardient based conterfactual generation\n",
        "    instance_tensor = torch.tensor(instance, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "    #parse instance components\n",
        "    behavioural_seq_tensor = instance_tensor[:120].reshape(1, 30, 4) # Reshape for single instance\n",
        "    textual_emb_tensor = instance_tensor[120:504].unsqueeze(0) # Add batch dimension\n",
        "    structured_features_tensor = instance_tensor[504:].unsqueeze(0) # Add batch dimension\n",
        "\n",
        "\n",
        "    optimizer = torch.optim.Adam([instance_tensor], lr=0.01)\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      #forward pass\n",
        "      behavioural_seq_batch = behavioural_seq_tensor.to(self.model.fusion[0].weight.device)\n",
        "      textual_emb_batch = textual_emb_tensor.to(self.model.fusion[0].weight.device)\n",
        "      structured_features_batch = structured_features_tensor.to(self.model.fusion[0].weight.device)\n",
        "\n",
        "\n",
        "      pred = self.model(behavioural_seq_batch, textual_emb_batch, structured_features_batch)\n",
        "\n",
        "      #Loss: minimize distance to target probabillity\n",
        "      loss = (pred - target_prob) ** 2\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if abs(pred.item() - target_prob) < 0.05:\n",
        "        break\n",
        "\n",
        "    return instance_tensor.detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KEIjv7pyVJyk"
      },
      "outputs": [],
      "source": [
        "#Evaluation metrics\n",
        "class ChurnEvaluator:\n",
        "  #comprehensive evaluation including business metrics\n",
        "\n",
        "  def __init__(self, avg_customer_values=100, churn_cost_multiplier=5):\n",
        "    self.avg_customer_values = avg_customer_values\n",
        "    self.churn_cost_multiplier = churn_cost_multiplier\n",
        "\n",
        "  def evaluate_model(self, y_true, y_pred_proba, y_pred_binary=None):\n",
        "    #Comprehensive model evaluation\n",
        "\n",
        "    if y_pred_binary is None:\n",
        "      y_pred_binary = (y_pred_proba > 0.5).astype(int)\n",
        "\n",
        "    #Traditional metrics\n",
        "    auc_score = roc_auc_score(y_true, y_pred_proba)\n",
        "    f1 = f1_score(y_true, y_pred_binary)\n",
        "    precision = precision_score(y_true, y_pred_binary)\n",
        "    recall = recall_score(y_true, y_pred_binary)\n",
        "\n",
        "    #Business metrics\n",
        "    expected_revenue_loss = self.calculate_expected_revenue_loss(y_true, y_pred_proba)\n",
        "    cost_reduction = self.calculate_cost_reduction(y_true, y_pred_binary)\n",
        "\n",
        "    #Precision at different thresholds\n",
        "    precision_at_k = self.precision_at_k(y_true, y_pred_proba, k_values=[10, 20, 30])\n",
        "\n",
        "    return{\n",
        "        'auc': auc_score,\n",
        "        'f1': f1,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'expected_revenue_loss': expected_revenue_loss,\n",
        "        'cost_reduction': cost_reduction,\n",
        "        'precision_at_k': precision_at_k\n",
        "    }\n",
        "\n",
        "  def calculate_expected_revenue_loss(self, y_true, y_pred_proba):\n",
        "    #Calculate expected revenue loss based on churn probabilities\n",
        "    expected_loss = np.sum(y_pred_proba * self.avg_customer_values)\n",
        "    actual_loss = np.sum(y_true * self.avg_customer_values)\n",
        "    return abs(expected_loss - actual_loss)\n",
        "\n",
        "  def calculate_cost_reduction(self, y_true, y_pred_binary):\n",
        "    #calculate cost reduction from intervention\n",
        "    #assume intervention costs 20% of customer value but saves 80% if successful\n",
        "    intervention_cost = 0.2 * self.avg_customer_values\n",
        "    save_rate = 0.8 # 80% of interventions successful\n",
        "\n",
        "    #True Positives: correctly identified churners\n",
        "    tp = np.sum((y_true == 1) & (y_pred_binary == 1))\n",
        "\n",
        "    #False Positives: incorrectly identified churners\n",
        "    fp = np.sum((y_true == 0) & (y_pred_binary == 1))\n",
        "\n",
        "    #cost of interventions\n",
        "    intervention_costs = (tp + fp) * intervention_cost\n",
        "\n",
        "    #revenue saved from successful interventions\n",
        "    revenue_saved = tp * save_rate * self.avg_customer_values\n",
        "\n",
        "    #net cost reduction\n",
        "    return revenue_saved - intervention_costs\n",
        "\n",
        "  def precision_at_k(self, y_true, y_pred_proba, k_values):\n",
        "    #calculate precision at top  k predictions\n",
        "    results = {}\n",
        "\n",
        "    #Sort by Prediction probability\n",
        "    sorted_indices = np.argsort(y_pred_proba)[::-1]\n",
        "\n",
        "    for k in k_values:\n",
        "      if k > len(y_true):\n",
        "        k = len(y_true)\n",
        "\n",
        "\n",
        "      top_k_indices = sorted_indices[:k]\n",
        "      top_k_true = y_true[top_k_indices]\n",
        "      precision_k = np.sum(top_k_true) / k\n",
        "      results[f'precision_at_{k}'] = precision_k\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "id": "SeMGDCRKqD3W",
        "collapsed": true,
        "outputId": "0babb826-698b-4f9d-ae37-cea5efa25298"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating synthetic data..\n",
            "Extracting textual features...\n",
            "Data prepared: 4000 train, 1000 test samples\n",
            "Training model...\n",
            "Epoch 0, Loss: 0.1715\n",
            "Epoch 10, Loss: 0.1330\n",
            "Epoch 20, Loss: 0.1313\n",
            "Training complete!\n",
            "Evaluating model...\n",
            "\n",
            "= Model Evaluation Results =\n",
            "AUC-ROC: 0.9876\n",
            "F1 Score: 0.9122\n",
            "Precision: 0.9278\n",
            "Recall: 0.8970\n",
            "Expected Revenue Loss: 835.83\n",
            "Cost Reduction: $15780.00\n",
            "Precision at K: {'precision_at_10': np.float32(1.0), 'precision_at_20': np.float32(1.0), 'precision_at_30': np.float32(1.0)}\n",
            "Generating explanations..\n",
            "Setting up explainability...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Cannot take a larger sample than population when 'replace=False'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-13-3988997870.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-13-3988997870.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    196\u001b[0m   \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m   \u001b[0;31m#generate explanations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m   \u001b[0mshap_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounterfactual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_explanations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m   \u001b[0;31m#plot training history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m   \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-13-3988997870.py\u001b[0m in \u001b[0;36mgenerate_explanations\u001b[0;34m(self, sample_size)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;31m#select random samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     \u001b[0msample_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m     \u001b[0msample_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombined_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
          ]
        }
      ],
      "source": [
        "from typing import Counter\n",
        "#TRAINING PIPELINE\n",
        "class ChurnPredictionPipeline:\n",
        "  #complete training and evaluation\n",
        "  def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    self.device = device\n",
        "    self.model = None\n",
        "    self.scaler = None\n",
        "    self.textual_extractor = None\n",
        "    self.explainer = None\n",
        "    self.evaluator = ChurnEvaluator()\n",
        "\n",
        "  def prepare_data(self, n_customers=10000, test_size=0.2):\n",
        "    #Generate and prepare multi-modal data\n",
        "    print(\"Generating synthetic data..\")\n",
        "\n",
        "    #Generate data\n",
        "    data_gen = DataGenerator(n_customers=n_customers)\n",
        "    behavioural_data, labels = data_gen.generate_behavioural_sequences()\n",
        "    textual_data = data_gen.generate_textual_data(labels)\n",
        "    structured_data = data_gen.generate_structured_data(labels)\n",
        "\n",
        "    #Extract textual features\n",
        "    print(\"Extracting textual features...\")\n",
        "    self.textual_extractor = TextualFeatureExtractor()\n",
        "    # Explicitly convert textual_data to a list of strings\n",
        "    textual_embeddings = self.textual_extractor.extract_features(textual_data.tolist())\n",
        "\n",
        "    #scale structured data\n",
        "    self.scaler = StandardScaler()\n",
        "    structured_data_scaled = self.scaler.fit_transform(structured_data)\n",
        "\n",
        "    #Train-test split\n",
        "    indices = np.arange(len(labels))\n",
        "    train_idx, test_idx = train_test_split(indices, test_size=test_size, stratify=labels, random_state=42)\n",
        "\n",
        "    #Prepare Datasets\n",
        "    self.train_data = {\n",
        "        'behavioural': torch.tensor(behavioural_data[train_idx], dtype=torch.float32),\n",
        "        'textual': textual_embeddings[train_idx],\n",
        "        'structured': torch.tensor(structured_data_scaled[train_idx], dtype=torch.float32),\n",
        "        'labels': torch.tensor(labels[train_idx], dtype=torch.float32)\n",
        "    }\n",
        "\n",
        "    self.test_data = {\n",
        "        'behavioural': torch.tensor(behavioural_data[test_idx], dtype=torch.float32),\n",
        "        'textual': textual_embeddings[test_idx],\n",
        "        'structured': torch.tensor(structured_data_scaled[test_idx], dtype=torch.float32),\n",
        "        'labels': torch.tensor(labels[test_idx], dtype=torch.float32)\n",
        "    }\n",
        "\n",
        "    print(f\"Data prepared: {len(train_idx)} train, {len(test_idx)} test samples\")\n",
        "    return self.train_data, self.test_data\n",
        "\n",
        "  def train_model(self, epochs=50, batch_size=64, learning_rate=0.001):\n",
        "    #Train the multi-modal churn prediction model\n",
        "    print(\"Training model...\")\n",
        "    #initialize model\n",
        "    self.model = MultiModalChurnPredictor(\n",
        "        behavioural_dim=4,\n",
        "        textual_dim=384,\n",
        "        structured_dim=6\n",
        "    ).to(self.device)\n",
        "\n",
        "    #loss and optimizer\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "\n",
        "    #Training Loop\n",
        "    train_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      self.model.train()\n",
        "      total_loss=0\n",
        "\n",
        "      #create mini-batches\n",
        "      n_samples = len(self.train_data['labels'])\n",
        "      indices = torch.randperm(n_samples)\n",
        "\n",
        "      for i in range(0, n_samples, batch_size):\n",
        "        batch_indices = indices[i:i+batch_size]\n",
        "\n",
        "        #get batch data\n",
        "        Behavioural_batch = self.train_data['behavioural'][batch_indices].to(self.device)\n",
        "        textual_batch = self.train_data['textual'][batch_indices].to(self.device)\n",
        "        structured_batch = self.train_data['structured'][batch_indices].to(self.device)\n",
        "        labels_batch = self.train_data['labels'][batch_indices].to(self.device)\n",
        "\n",
        "        #forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = self.model(Behavioural_batch, textual_batch, structured_batch)\n",
        "        loss = criterion(outputs.squeeze(), labels_batch)\n",
        "\n",
        "        #backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "      avg_loss = total_loss / (n_samples // batch_size)\n",
        "      train_losses.append(avg_loss)\n",
        "\n",
        "      if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "    return train_losses\n",
        "\n",
        "  def evaluate_model(self):\n",
        "    #evaluate the trained model\n",
        "    print(\"Evaluating model...\")\n",
        "\n",
        "    self.model.eval()\n",
        "    with torch.no_grad():\n",
        "      #get predictions\n",
        "      behavioural_test = self.test_data['behavioural'].to(self.device)\n",
        "      textual_test = self.test_data['textual'].to(self.device)\n",
        "      structured_test = self.test_data['structured'].to(self.device)\n",
        "\n",
        "      predictions = self.model(behavioural_test, textual_test, structured_test)\n",
        "      predictions = predictions.squeeze().cpu().numpy()\n",
        "\n",
        "      #true labels\n",
        "      y_true = self.test_data['labels'].cpu().numpy()\n",
        "\n",
        "      #evaluate\n",
        "      results = self.evaluator.evaluate_model(y_true, predictions)\n",
        "\n",
        "      #print results\n",
        "      print(\"\\n= Model Evaluation Results =\")\n",
        "      print(f\"AUC-ROC: {results['auc']:.4f}\")\n",
        "      print(f\"F1 Score: {results['f1']:.4f}\")\n",
        "      print(f\"Precision: {results['precision']:.4f}\")\n",
        "      print(f\"Recall: {results['recall']:.4f}\")\n",
        "      print(f\"Expected Revenue Loss: {results['expected_revenue_loss']:.2f}\")\n",
        "      print(f\"Cost Reduction: ${results['cost_reduction']:.2f}\")\n",
        "      print(f\"Precision at K: {results['precision_at_k']}\")\n",
        "\n",
        "      return results, predictions\n",
        "\n",
        "  def setup_explainability(self):\n",
        "      #Setup explainability components\n",
        "      print(\"Setting up explainability...\")\n",
        "\n",
        "      #prepare flattened data for SHAP\n",
        "      behavioural_flat = self.test_data['behavioural'].reshape(len( self.test_data['behavioural']), -1)\n",
        "      textual_flat = self.test_data['textual']\n",
        "      structured_flat = self.test_data['structured']\n",
        "\n",
        "\n",
        "      #combine all features\n",
        "      combined_features = torch.cat([behavioural_flat, textual_flat, structured_flat], dim=1)\n",
        "\n",
        "      #create feature names\n",
        "      feature_names = []\n",
        "      for i in range(30): #30 time steps\n",
        "        for j in range(4): #4 behavioural features\n",
        "          feature_names.append(f'behavioural_t{i}_f{j}')\n",
        "      for i in range(384): #384 textual embedding dimension\n",
        "          feature_names.append(f'textual_emb_{i}')\n",
        "      for i, name in enumerate(['tenure', 'monthly_charges', 'support_tickets', 'plan_changes', 'auto_pay', 'contract_type']):\n",
        "        feature_names.append(f'structured_{name}')\n",
        "\n",
        "      return combined_features.numpy(), feature_names\n",
        "\n",
        "\n",
        "  def generate_explanations(self, sample_size=10):\n",
        "    #Generate explanations for sample predictions\n",
        "    print(\"Generating explanations..\")\n",
        "\n",
        "    # Get combined features and feature names\n",
        "    combined_features =  self.setup_explainability()\n",
        "\n",
        "    #select random samples\n",
        "    sample_indices = np.random.choice(len(combined_features), sample_size, replace=False)\n",
        "    sample_data = combined_features[sample_indices]\n",
        "\n",
        "    #Generate SHAP explanations\n",
        "    shap_values = self.explainer.explain_predictions(sample_data)\n",
        "\n",
        "\n",
        "    #generate conterfactual for one sample\n",
        "    counterfactual = self.explainer.generate_counterfactuals(sample_data[0])\n",
        "\n",
        "    return shap_values, counterfactual\n",
        "\n",
        "#MAIN EXECUTION\n",
        "def main():\n",
        "  #main execution function\n",
        "  #initialize pipeline\n",
        "  pipeline = ChurnPredictionPipeline()\n",
        "  #prepare data\n",
        "  train_data, test_data = pipeline.prepare_data(n_customers=5000)\n",
        "  #train model\n",
        "  train_losses = pipeline.train_model(epochs=30, batch_size=32)\n",
        "  #evaluate model\n",
        "  results, predictions = pipeline.evaluate_model()\n",
        "  #generate explanations\n",
        "  shap_values, counterfactual = pipeline.generate_explanations(sample_size=5)\n",
        "  #plot training history\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(train_losses)\n",
        "  plt.title('Training Loss Over Time')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.show()\n",
        "\n",
        "  #plot prediction distribution\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.hist(predictions, bins=50, alpha=0.7, label='Predictions')\n",
        "  plt.axvline(x=0.5, color='red', linestyle='--', label='Decision Threshold')\n",
        "  plt.title('Distribution of Churn Predictions')\n",
        "  plt.xlabel('Churn Probability')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  print(\"\\n== Pipeline Completed Successfully ==\")\n",
        "  print(\"The multi-modal churn Prediction system is ready for deployment\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}